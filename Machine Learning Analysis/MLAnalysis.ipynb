{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the required packages and set up the Spark session. This would not be required if using DataBricks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark;\n",
    "spark = SparkSession.builder.appName('A2').getOrCreate();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: We will be using Spark for GridSearchCV only. This appears to be the only sklearn method currently supported by Spark. The remaining code will be executed using Pandas dataframes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the CSV file (n=1000 samples) containing our manual labels as the target vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (spark.read\n",
    "  .format(\"csv\")\n",
    "  .option(\"header\", \"true\")\n",
    "  .option(\"inferSchema\", \"true\")\n",
    "  .option(\"multiline\", \"true\")\n",
    "  .option(\"quote\", '\"')  \n",
    "  .option(\"escape\", \"\\\\\")\n",
    "  .option(\"escape\", '\"')\n",
    "  .load(\"GH-React.csv\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"./GH-React.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape:\", df.shape)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Pipeline\n",
    "### Stage 1\n",
    "The preprocess() function is defined below. It takes in a String formatted as Markdown from GitHub and pre-processes it to return a new string ready for the next stages in our ML Pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocess(text):\n",
    "    stripped = text.lower()\n",
    "\n",
    "    # remove all headings, bold text, and HTML comments from the Markdown text.\n",
    "    # These items have all been used by the React team in their issue templates on GitHub\n",
    "    headings_pattern = r'(<=\\s|^)#{1,6}(.*?)$'\n",
    "    bold_pattern = r'\\*\\*(.+?)\\*\\*(?!\\*)'\n",
    "    comments_pattern = r'<!--((.|\\n)*?)-->'\n",
    "    combined_pattern = r'|'.join((headings_pattern, bold_pattern, comments_pattern))\n",
    "\n",
    "    stripped = re.sub(combined_pattern, '', stripped)\n",
    "\n",
    "    # find all URLs in the string, and then remove the final directory from each to leave the general URL form\n",
    "    # there may be useful patterns based on what URLs issues are commonly linking to\n",
    "    url_pattern = re.compile(r'(https?://[^\\s]+)')\n",
    "    for url in re.findall(url_pattern, stripped):\n",
    "        new_url = url.rsplit(\"/\", 1)[0]\n",
    "        stripped = stripped.replace(url, new_url)\n",
    "\n",
    "    non_alpha_pattern = r'[^A-Za-z ]+'\n",
    "    stripped = re.sub(non_alpha_pattern, '', stripped)    \n",
    "    \n",
    "    return ' '.join(stripped.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bug or undefined behaviourdoingreactchildrentoarray reactdomcreateportalfails withobjects are not valid as a react child found object with keys typeof key children containerinfo implementation if you meant to render a collection of children use an array insteadnamely the following complete snippet failsjsximport react from reactimport render createportal from reactdomconst renderchildren children children reactchildrentoarraychildren return hrenders children with toarray childrenhconst app renderchildren namecodesandbox createportaldivrendered in portaldiv documentgetelementbyidportal renderchildrenrenderapp documentgetelementbyidrootwhile the following one which wraps the portal in another element works just finejsximport react from reactimport render createportal from reactdomconst renderchildren children children reactchildrentoarraychildren return hrenders children with toarray childrenhconst app renderchildren namecodesandbox div createportaldivrendered in portaldiv documentgetelementbyidportal div renderchildrenrenderapp documentgetelementbyidrooti am aware that createportal is a new feature but in the best case scenario it should be possible to use it everywhere other valid nodes are acceptedthe same thing is happening for reactcloneelementreactdomcreateportal its probably weird to try and clone a portal but maybe we should specify in the createportal documentation that it cannot be cloned at least for now should i open a pr for thatlet me know your thoughtsim using react'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = df['body'][4]\n",
    "preprocess(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 2\n",
    "Split the data into training (80%) and validation(20%) sets. We will stratify based on the label since our dataset is imbalanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['Target']\n",
    "data = df.drop(['Target'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 15)\n",
      "(200, 15)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_train, data_val, y_train, y_val = train_test_split(data, y, train_size=0.8, stratify=y, random_state=1)\n",
    "\n",
    "print(data_train.shape)\n",
    "print(data_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 3\n",
    "Create a TF-IDF features matrix using TfidfVectorizer from sklearn applied to the title and body of each issue.\n",
    "\n",
    "We will additionally add in the feature 'author_association' from the GitHub issue, as there may be a correlation between Members/Collaborators/Contributors submitting more valid bugs/feature requests than \"None\" users.\n",
    "\n",
    "While lemmatization could have been done earlier in the pre-processsing stage, it is more efficient to lemmatize at this point in a custom_tokenizer() function passed to TfidfVectorizer since tokenization is part of both processses.\n",
    "\n",
    "First, define the tokenizer and vectorizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install spacy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# create a custom tokenizer using the spacy document processing pipeline\n",
    "def custom_tokenizer(document):\n",
    "    ppd = preprocess(document)\n",
    "    doc = nlp(ppd)\n",
    "    return [token.lemma_ for token in doc]\n",
    "\n",
    "vect = TfidfVectorizer(tokenizer=custom_tokenizer, ngram_range=(1, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the features matrix:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Need to convert the below effort into a columnular transformer. It cannot be a manual effort if we want to use this in grid search later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the title and body into one column\n",
    "text = (data_train['title'] + ' ' + data_train['body']).values.astype('U')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create text_features using our vectorizer\n",
    "text_features = vect.fit_transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features with lowest idf:\n",
      "['the' 'be' 'to' 'not' 'in' 'a' 'and' 'react' 'this' 'I' 'it' 'of' 'do'\n",
      " 'with' 'use' 'that' 'for' 'version' 'bug' 'when']\n",
      "Features with highest idf:\n",
      "['getcomponentname to' 'getcomponentnamefromfiber'\n",
      " 'getcomponentnamefromfiber although' 'getcomponentnamefromtype'\n",
      " 'getcomponentnamefromtype getcomponentnamefromfiber'\n",
      " 'getcomponentnamefromtype in' 'getcomponentthisgetcomponent'\n",
      " 'getcomponentthisgetcomponent export' 'getcssmodulelocalident'\n",
      " 'getcssmodulelocalident lessloader' 'getcurrenttimeshouldyieldtohost'\n",
      " 'getcurrenttimeshouldyieldtohost return' 'getcurrentvalue'\n",
      " 'getcurrentvalue chromeextensionfmkadmapgofadopljbjfkapdkoienihibuildmainj'\n",
      " 'getcursor consolelogccstatestate' 'getcursor const'\n",
      " 'getchildhostcontextcontext fibertype' 'getdata const'\n",
      " 'getchildhostcontextcontext' 'getchildhostcontext hi' 'get theme'\n",
      " 'get throw' 'get to' 'get typeerror' 'get use' 'get value' 'get with'\n",
      " 'get zero' 'getaudiocontext' 'getaudiocontext setaudiotrack'\n",
      " 'getaudiocontext usestatenull'\n",
      " 'getaudiocontextmediastreamaudiosourcenodeselectedaudiotrack'\n",
      " 'getaudiocontextmediastreamaudiosourcenodeselectedaudiotrack else'\n",
      " 'getboundingclientrect' 'getboundingclientrect to' 'getbytestid'\n",
      " 'getbytestid rendermycomponent' 'getbytestidtest'\n",
      " 'getbytestidtest expecttestelementtextcontenttoequalbir' 'getchartdata'\n",
      " 'getchartdata chromeextensionfmkadmapgofadopljbjfkapdkoienihibuildmainj'\n",
      " 'getchildhostcontext' 'getchildhostcontext function'\n",
      " 'getchildhostcontext it' 'getdata method' 'getdata methodhowever'\n",
      " 'getdata return' 'gete datacode' 'geterror' 'geterror return'\n",
      " 'getjsconst' 'getjsconst examplecomponent' 'getkey' 'getkey item'\n",
      " 'getloading' 'getloading return' 'getlocalident'\n",
      " 'getlocalident getcssmodulelocalident' 'getmodify' 'getmodify the'\n",
      " 'getparentref' 'getparentref consolelogref' 'getparentref getref'\n",
      " 'getparentref return' 'getparentref useeffect' 'getparentrefgetparentref'\n",
      " 'getparentrefgetparentref inner' 'getparentrefgetparentref reactfragment'\n",
      " 'getplugin' 'getplugin homepraveenforgenodereactscriptsrollupbuildj'\n",
      " 'gete' 'getderivedstatefromprop would' 'getderivedstatefromprop with'\n",
      " 'getderivedstatefromprop unsafecomponentwillmount' 'getdata some'\n",
      " 'getdatafunc' 'getdatafunc return' 'getdatafuncdatasource'\n",
      " 'getdatafuncdatasource prop' 'getdatafuncdatasource thisprop' 'getdeal'\n",
      " 'getdeal salevehicleprice' 'getderivedstatefromerror be'\n",
      " 'getderivedstatefromerror do' 'getderivedstatefromerror in'\n",
      " 'get stuckcan' 'getderivedstatefromerrorerror'\n",
      " 'getderivedstatefromprop and' 'getderivedstatefromprop as'\n",
      " 'getderivedstatefromprop do'\n",
      " 'getderivedstatefromprop featuregetderivedstatefromprop'\n",
      " 'getderivedstatefromprop function' 'getderivedstatefromprop method'\n",
      " 'getderivedstatefromprop not' 'getderivedstatefromprop present'\n",
      " 'getderivedstatefromprop prop' 'getderivedstatefromprop this'\n",
      " 'getderivedstatefromprop too' 'getderivedstatefromerrorerror return'\n",
      " 'zoomable view']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "sorted_by_idf = np.argsort(vect.idf_)\n",
    "feature_names = np.array(vect.get_feature_names())\n",
    "\n",
    "print(\"Features with lowest idf:\\n{}\".format(feature_names[sorted_by_idf[:20]]))\n",
    "print(\"Features with highest idf:\\n{}\".format(feature_names[sorted_by_idf[-100:]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# use one hot encoder to transform the author_association to a feature set\n",
    "ohe = OneHotEncoder()\n",
    "author_association = ohe.fit_transform(data_train['author_association'].to_numpy().reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack\n",
    "\n",
    "# create our final features matrix by combining both sets\n",
    "X_train = hstack((text_features, author_association))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of feature matrix:\", X_train.shape)\n",
    "print(\"Shape of target vector:\", y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: See if we can visualize our text features to make sure it seems logical. Take an example from Chapter 7 in ML book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 4\n",
    "Use cross-validate with our training set to test our model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Search for optimizing model\n",
    "\n",
    "Use grid search to find potentially better model parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install joblibspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#TODO: Need to make the features vector matrix creation into a pipeline or function so we can feed it into grid search. It cannot be a manual effort.\n",
    "\n",
    "could use the columnular transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import numpy as np\n",
    "\n",
    "# Use spark_sklearnâ€™s grid search instead:\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from joblibspark import register_spark\n",
    "from sklearn.utils import parallel_backend\n",
    "\n",
    "register_spark() # register spark backend\n",
    "\n",
    "pipe = make_pipeline(TfidfVectorizer(tokenizer=custom_tokenizer), LogisticRegression())\n",
    "\n",
    "param_grid = {\"logisticregression__C\": [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "                \"tfidfvectorizer__ngram_range\": [(1, 1), (1, 2), (1, 3)]}\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid, cv=5)\n",
    "grid.fit(data_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best parameters: {}\".format(grid.best_params_))\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 5\n",
    "Validate our chosen model against the validation set we saved in Stage 3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ENSF-612",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "758f2f2736c7fa9cea9c3ba7e462cc50ad01011ab3fd1554d8a168a53b5df95b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
