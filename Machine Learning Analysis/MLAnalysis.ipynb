{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the required packages and set up the Spark session. This would not be required if using DataBricks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages/pyspark'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark;\n",
    "spark = SparkSession.builder.appName('A2').getOrCreate();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: We will be using Spark for GridSearchCV only. This appears to be the only sklearn method currently supported by Spark. The remaining code will be executed using Pandas dataframes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the CSV file (n=1000 samples) containing our manual labels as the target vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (spark.read\n",
    "  .format(\"csv\")\n",
    "  .option(\"header\", \"true\")\n",
    "  .option(\"inferSchema\", \"true\")\n",
    "  .option(\"multiline\", \"true\")\n",
    "  .option(\"quote\", '\"')  \n",
    "  .option(\"escape\", \"\\\\\")\n",
    "  .option(\"escape\", '\"')\n",
    "  .load(\"GH-React.csv\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"./GH-React.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (1000, 16)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['html_url', 'number', 'title', 'labels', 'state', 'locked', 'milestone',\n",
       "       'comments', 'created_at', 'updated_at', 'closed_at',\n",
       "       'author_association', 'state_reason', 'assignee.login', 'body',\n",
       "       'Target'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Shape:\", df.shape)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Pipeline\n",
    "### Stage 1\n",
    "The preprocess() function is defined below. It takes in a String formatted as Markdown from GitHub and pre-processes it to return a new string ready for the next stages in our ML Pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocess(text):\n",
    "    stripped = text.lower()\n",
    "\n",
    "    # remove all headings, bold text, and HTML comments from the Markdown text.\n",
    "    # These items have all been used by the React team in their issue templates on GitHub\n",
    "    headings_pattern = r'(<=\\s|^)#{1,6}(.*?)$'\n",
    "    bold_pattern = r'\\*\\*(.+?)\\*\\*(?!\\*)'\n",
    "    comments_pattern = r'<!--((.|\\n)*?)-->'\n",
    "    combined_pattern = r'|'.join((headings_pattern, bold_pattern, comments_pattern))\n",
    "\n",
    "    stripped = re.sub(combined_pattern, '', stripped)\n",
    "\n",
    "    # find all URLs in the string, and then remove the final directory from each to leave the general URL form\n",
    "    # there may be useful patterns based on what URLs issues are commonly linking to\n",
    "    url_pattern = re.compile(r'(https?://[^\\s]+)')\n",
    "    for url in re.findall(url_pattern, stripped):\n",
    "        new_url = url.rsplit(\"/\", 1)[0]\n",
    "        stripped = stripped.replace(url, new_url)\n",
    "    \n",
    "    non_alpha_pattern = r'[^A-Za-z\\n ]+'\n",
    "    stripped = re.sub(non_alpha_pattern, '', stripped)\n",
    "    \n",
    "    return stripped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\n\\nbug or undefined behaviour\\n\\n\\n\\ndoing\\n\\nreactchildrentoarray\\n  reactdomcreateportal\\n\\n\\n\\nfails with\\n\\nobjects are not valid as a react child found object with keys typeof key children containerinfo implementation if you meant to render a collection of children use an array instead\\n\\n\\nnamely the following complete snippet fails\\n\\njsx\\nimport react from react\\nimport  render createportal  from reactdom\\n\\nconst renderchildren   children   \\n  children  reactchildrentoarraychildren\\n  return hrenders children with toarray childrenh\\n\\n\\n\\nconst app     \\n  renderchildren namecodesandbox\\n    createportaldivrendered in portaldiv documentgetelementbyidportal\\n  renderchildren\\n\\n\\nrenderapp  documentgetelementbyidroot\\n\\n\\nwhile the following one which wraps the portal in another element works just fine\\n\\njsx\\nimport react from react\\nimport  render createportal  from reactdom\\n\\nconst renderchildren   children   \\n  children  reactchildrentoarraychildren\\n  return hrenders children with toarray childrenh\\n\\n\\n\\nconst app     \\n  renderchildren namecodesandbox\\n    div\\n        createportaldivrendered in portaldiv documentgetelementbyidportal\\n    div\\n  renderchildren\\n\\n\\nrenderapp  documentgetelementbyidroot\\n\\n\\n\\ni am aware that createportal is a new feature but in the best case scenario it should be possible to use it everywhere other valid nodes are accepted\\n\\nthe same thing is happening for reactcloneelementreactdomcreateportal  its probably weird to try and clone a portal    but maybe we should specify in the createportal documentation that it cannot be cloned at least for now should i open a pr for that\\n\\nlet me know your thoughts\\n\\n\\n\\nim using react '"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = df['body'][4]\n",
    "preprocess(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 2\n",
    "Split the data into training (80%) and validation(20%) sets. We will stratify based on the label since our dataset is imbalanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['Target']\n",
    "data = df.drop(['Target'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 15)\n",
      "(200, 15)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_train, data_val, y_train, y_val = train_test_split(data, y, train_size=0.8, stratify=y, random_state=1)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 3\n",
    "Create a TF-IDF features matrix using TfidfVectorizer from sklearn applied to the title and body of each issue.\n",
    "\n",
    "We will additionally add in the feature 'author_association' from the GitHub issue, as there may be a correlation between Members/Collaborators/Contributors submitting more valid bugs/feature requests than \"None\" users.\n",
    "\n",
    "While lemmatization could have been done earlier in the pre-processsing stage, it is more efficient to lemmatize at this point in a custom_tokenizer() function passed to TfidfVectorizer since tokenization is part of both processses.\n",
    "\n",
    "First, define the tokenizer and vectorizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages (3.4.3)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages (from spacy) (1.0.3)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages (from spacy) (0.10.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages (from spacy) (1.21.5)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages (from spacy) (0.7.0)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages (from spacy) (8.1.5)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages (from spacy) (2.4.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages (from spacy) (2.28.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages (from spacy) (4.64.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages (from spacy) (1.0.9)\n",
      "Requirement already satisfied: setuptools in /home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages (from spacy) (63.4.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages (from spacy) (1.10.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages (from spacy) (21.3)\n",
      "Requirement already satisfied: jinja2 in /home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages (from spacy) (2.0.7)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages (from spacy) (3.0.10)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages (from spacy) (3.0.8)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages (from spacy) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages (from packaging>=20.0->spacy) (3.0.9)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.3.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.9.24)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.12)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.0.3)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.7.9)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages (from typer<0.8.0,>=0.3.0->spacy) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages (from jinja2->spacy) (2.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting en-core-web-sm==3.4.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.4.1/en_core_web_sm-3.4.1-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages (from en-core-web-sm==3.4.1) (3.4.3)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.7)\n",
      "Requirement already satisfied: jinja2 in /home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.1.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.8)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.28.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.3.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.4.5)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.64.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.10.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (21.3)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.10)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.10.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.10.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.9)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.8)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (8.1.5)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.3)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.21.5)\n",
      "Requirement already satisfied: setuptools in /home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (63.4.1)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.7.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.9)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2022.9.24)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.26.12)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.4)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.0.3)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.7.9)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages (from typer<0.8.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.1.1)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "%pip install spacy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.pipeline import make_pipeline\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# create a custom tokenizer using the spacy document processing pipeline\n",
    "def custom_tokenizer(document):\n",
    "    ppd = preprocess(document)\n",
    "    doc = nlp(ppd)\n",
    "    return [token.lemma_ for token in doc]\n",
    "\n",
    "vect = TfidfVectorizer(tokenizer=custom_tokenizer, ngram_range=(1, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the features matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the title and body into one column\n",
    "text = (data_train['title'] + ' ' + data_train['body']).values.astype('U')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create text_features using our vectorizer\n",
    "text_features = vect.fit_transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# use one hot encoder to transform the author_association to a feature set\n",
    "ohe = OneHotEncoder()\n",
    "author_association = ohe.fit_transform(data_train['author_association'].to_numpy().reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack\n",
    "\n",
    "# create our final features matrix by combining both sets\n",
    "X_train = hstack((text_features, author_association))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of feature matrix: (800, 63479)\n",
      "Shape of target vector: (800,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of feature matrix:\", X_train.shape)\n",
    "print(\"Shape of target vector:\", y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 4\n",
    "Use cross-validate with our training set to test our model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Search for optimizing model\n",
    "\n",
    "Use grid search to find potentially better model parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting joblibspark\n",
      "  Downloading joblibspark-0.5.0-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: joblib>=0.14 in /home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages (from joblibspark) (1.1.0)\n",
      "Installing collected packages: joblibspark\n",
      "Successfully installed joblibspark-0.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install joblibspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "\nAll the 90 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n90 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages/sklearn/pipeline.py\", line 382, in fit\n    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n  File \"/home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1138, in fit\n    X, y = self._validate_data(\n  File \"/home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages/sklearn/base.py\", line 596, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n  File \"/home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 1092, in check_X_y\n    check_consistent_length(X, y)\n  File \"/home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 387, in check_consistent_length\n    raise ValueError(\nValueError: Found input variables with inconsistent numbers of samples: [15, 640]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [135], line 19\u001b[0m\n\u001b[1;32m     15\u001b[0m param_grid \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mlogisticregression__C\u001b[39m\u001b[39m\"\u001b[39m: [\u001b[39m0.001\u001b[39m, \u001b[39m0.01\u001b[39m, \u001b[39m0.1\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m10\u001b[39m, \u001b[39m100\u001b[39m],\n\u001b[1;32m     16\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mtfidfvectorizer__ngram_range\u001b[39m\u001b[39m\"\u001b[39m: [(\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m), (\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m), (\u001b[39m1\u001b[39m, \u001b[39m3\u001b[39m)]}\n\u001b[1;32m     18\u001b[0m grid \u001b[39m=\u001b[39m GridSearchCV(pipe, param_grid, cv\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m)\n\u001b[0;32m---> 19\u001b[0m grid\u001b[39m.\u001b[39;49mfit(data_train, y_train)\n",
      "File \u001b[0;32m~/anaconda3/envs/ENSF-612/lib/python3.9/site-packages/sklearn/model_selection/_search.py:875\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    869\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_results(\n\u001b[1;32m    870\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    871\u001b[0m     )\n\u001b[1;32m    873\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n\u001b[0;32m--> 875\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_search(evaluate_candidates)\n\u001b[1;32m    877\u001b[0m \u001b[39m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    878\u001b[0m \u001b[39m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    879\u001b[0m first_test_score \u001b[39m=\u001b[39m all_out[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtest_scores\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/ENSF-612/lib/python3.9/site-packages/sklearn/model_selection/_search.py:1375\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1373\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_search\u001b[39m(\u001b[39mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1374\u001b[0m     \u001b[39m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1375\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparam_grid))\n",
      "File \u001b[0;32m~/anaconda3/envs/ENSF-612/lib/python3.9/site-packages/sklearn/model_selection/_search.py:852\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    845\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m!=\u001b[39m n_candidates \u001b[39m*\u001b[39m n_splits:\n\u001b[1;32m    846\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    847\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mcv.split and cv.get_n_splits returned \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    848\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39minconsistent results. Expected \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    849\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39msplits, got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(n_splits, \u001b[39mlen\u001b[39m(out) \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m n_candidates)\n\u001b[1;32m    850\u001b[0m     )\n\u001b[0;32m--> 852\u001b[0m _warn_or_raise_about_fit_failures(out, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49merror_score)\n\u001b[1;32m    854\u001b[0m \u001b[39m# For callable self.scoring, the return type is only know after\u001b[39;00m\n\u001b[1;32m    855\u001b[0m \u001b[39m# calling. If the return type is a dictionary, the error scores\u001b[39;00m\n\u001b[1;32m    856\u001b[0m \u001b[39m# can now be inserted with the correct key. The type checking\u001b[39;00m\n\u001b[1;32m    857\u001b[0m \u001b[39m# of out will be done in `_insert_error_scores`.\u001b[39;00m\n\u001b[1;32m    858\u001b[0m \u001b[39mif\u001b[39;00m callable(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscoring):\n",
      "File \u001b[0;32m~/anaconda3/envs/ENSF-612/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:367\u001b[0m, in \u001b[0;36m_warn_or_raise_about_fit_failures\u001b[0;34m(results, error_score)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[39mif\u001b[39;00m num_failed_fits \u001b[39m==\u001b[39m num_fits:\n\u001b[1;32m    361\u001b[0m     all_fits_failed_message \u001b[39m=\u001b[39m (\n\u001b[1;32m    362\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mAll the \u001b[39m\u001b[39m{\u001b[39;00mnum_fits\u001b[39m}\u001b[39;00m\u001b[39m fits failed.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    363\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIt is very likely that your model is misconfigured.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    364\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou can try to debug the error by setting error_score=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mraise\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    365\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBelow are more details about the failures:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mfit_errors_summary\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    366\u001b[0m     )\n\u001b[0;32m--> 367\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(all_fits_failed_message)\n\u001b[1;32m    369\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    370\u001b[0m     some_fits_failed_message \u001b[39m=\u001b[39m (\n\u001b[1;32m    371\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mnum_failed_fits\u001b[39m}\u001b[39;00m\u001b[39m fits failed out of a total of \u001b[39m\u001b[39m{\u001b[39;00mnum_fits\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    372\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThe score on these train-test partitions for these parameters\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    376\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBelow are more details about the failures:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mfit_errors_summary\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    377\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: \nAll the 90 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n90 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages/sklearn/pipeline.py\", line 382, in fit\n    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n  File \"/home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1138, in fit\n    X, y = self._validate_data(\n  File \"/home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages/sklearn/base.py\", line 596, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n  File \"/home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 1092, in check_X_y\n    check_consistent_length(X, y)\n  File \"/home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 387, in check_consistent_length\n    raise ValueError(\nValueError: Found input variables with inconsistent numbers of samples: [15, 640]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import numpy as np\n",
    "\n",
    "# Use spark_sklearn’s grid search instead:\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from joblibspark import register_spark\n",
    "from sklearn.utils import parallel_backend\n",
    "\n",
    "register_spark() # register spark backend\n",
    "\n",
    "pipe = make_pipeline(TfidfVectorizer(tokenizer=custom_tokenizer), LogisticRegression())\n",
    "\n",
    "param_grid = {\"logisticregression__C\": [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "                \"tfidfvectorizer__ngram_range\": [(1, 1), (1, 2), (1, 3)]}\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid, cv=5)\n",
    "grid.fit(data_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best parameters: {}\".format(grid.best_params_))\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 5\n",
    "Validate our chosen model against the validation set we saved in Stage 3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ENSF-612",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "758f2f2736c7fa9cea9c3ba7e462cc50ad01011ab3fd1554d8a168a53b5df95b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
