{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the required packages and set up the Spark session. This would not be required if using DataBricks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages/pyspark'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark;\n",
    "spark = SparkSession.builder.appName('A2').getOrCreate();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: We will be using Spark for GridSearchCV only. This appears to be the only sklearn method currently supported by Spark. The remaining code will be executed using Pandas dataframes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the CSV file (n=1000 samples) containing our manual labels as the target vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (spark.read\n",
    "  .format(\"csv\")\n",
    "  .option(\"header\", \"true\")\n",
    "  .option(\"inferSchema\", \"true\")\n",
    "  .option(\"multiline\", \"true\")\n",
    "  .option(\"quote\", '\"')  \n",
    "  .option(\"escape\", \"\\\\\")\n",
    "  .option(\"escape\", '\"')\n",
    "  .load(\"GH-React.csv\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"./GH-React.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (1000, 16)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['html_url', 'number', 'title', 'labels', 'state', 'locked', 'milestone',\n",
       "       'comments', 'created_at', 'updated_at', 'closed_at',\n",
       "       'author_association', 'state_reason', 'assignee.login', 'body',\n",
       "       'Target'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Shape:\", df.shape)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Pipeline\n",
    "### Stage 1\n",
    "The preprocess() function is defined below. It takes in a String formatted as Markdown from GitHub and pre-processes it to return a new string ready for the next stages in our ML Pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocess(text):\n",
    "    stripped = text.lower()\n",
    "\n",
    "    # remove all headings, bold text, and HTML comments from the Markdown text.\n",
    "    # These items have all been used by the React team in their issue templates on GitHub\n",
    "    headings_pattern = r'(<=\\s|^)#{1,6}(.*?)$'\n",
    "    bold_pattern = r'\\*\\*(.+?)\\*\\*(?!\\*)'\n",
    "    comments_pattern = r'<!--((.|\\n)*?)-->'\n",
    "    combined_pattern = r'|'.join((headings_pattern, bold_pattern, comments_pattern))\n",
    "\n",
    "    stripped = re.sub(combined_pattern, '', stripped)\n",
    "\n",
    "    # find all URLs in the string, and then remove the final directory from each to leave the general URL form\n",
    "    # there may be useful patterns based on what URLs issues are commonly linking to\n",
    "    url_pattern = re.compile(r'(https?://[^\\s]+)')\n",
    "    for url in re.findall(url_pattern, stripped):\n",
    "        new_url = url.rsplit(\"/\", 1)[0]\n",
    "        stripped = stripped.replace(url, new_url)\n",
    "    \n",
    "    non_alpha_pattern = r'[^A-Za-z\\n ]+'\n",
    "    stripped = re.sub(non_alpha_pattern, '', stripped)\n",
    "    \n",
    "    return stripped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = df['body'][1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\nbug\\n\\n\\na specific order of unmounting and remounting unstablecreatereturns from reactcallreturn causes an invariant violation in unmounthostcomponents\\n\\n\\nthe following sandbox example crashes with an invariant violation when both the min and cycle props are odd numbers greater than zero\\n\\nhttpscodesandboxios\\n\\n\\nthe app does not crash and cycles the number of items in the list\\n\\n\\nreact and reactdom versions  and newer reactcallreturn version \\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 2\n",
    "Create a TF-IDF features matrix using TfidfVectorizer from sklearn applied to the title and body of each issue.\n",
    "\n",
    "We will additionally add in the feature 'author_association' from the GitHub issue, as there may be a correlation between Members/Collaborators/Contributors submitting more valid bugs/feature requests than \"None\" users.\n",
    "\n",
    "While lemmatization could have been done earlier in the pre-processsing stage, it is more efficient to lemmatize at this point in a custom_tokenizer() function passed to TfidfVectorizer since tokenization is part of both processses.\n",
    "\n",
    "First, define the tokenizer and vectorizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not run yet. I'm not sure if we want to use this rexex tokenizer instead of built in lemma tokenizer\n",
    "\n",
    "# Technicality: we want to use the regexp-based tokenizer\n",
    "# that is used by CountVectorizer and only use the lemmatization\n",
    "# from spacy. To this end, we replace en_nlp.tokenizer (the spacy tokenizer)\n",
    "# with the regexp-based tokenization.\n",
    "import re\n",
    "# regexp used in CountVectorizer\n",
    "regexp = re.compile('(?u)\\\\b\\\\w\\\\w+\\\\b')\n",
    "# load spacy language model and save old tokenizer\n",
    "en_nlp = spacy.load('en')\n",
    "old_tokenizer = en_nlp.tokenizer\n",
    "# replace the tokenizer with the preceding regexp\n",
    "en_nlp.tokenizer = lambda string: old_tokenizer.tokens_from_list(\n",
    "regexp.findall(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages (3.4.3)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages (from spacy) (1.21.5)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages (from spacy) (1.10.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages (from spacy) (21.3)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages (from spacy) (2.4.5)\n",
      "Requirement already satisfied: jinja2 in /home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages (from spacy) (4.64.1)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages (from spacy) (0.7.0)\n",
      "Requirement already satisfied: setuptools in /home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages (from spacy) (63.4.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages (from spacy) (3.0.8)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages (from spacy) (2.0.7)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages (from spacy) (1.0.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages (from spacy) (2.28.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages (from spacy) (1.0.9)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages (from spacy) (0.10.1)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages (from spacy) (0.10.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages (from spacy) (3.0.10)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages (from spacy) (8.1.5)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages (from packaging>=20.0->spacy) (3.0.9)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.12)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.9.24)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.0.3)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages (from typer<0.8.0,>=0.3.0->spacy) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages (from jinja2->spacy) (2.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting en-core-web-sm==3.4.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.4.1/en_core_web_sm-3.4.1-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages (from en-core-web-sm==3.4.1) (3.4.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.64.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.9)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (21.3)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.8)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.21.5)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.8)\n",
      "Requirement already satisfied: jinja2 in /home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.1.2)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.7.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.10.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.28.1)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (8.1.5)\n",
      "Requirement already satisfied: setuptools in /home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (63.4.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.3.0)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.10.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.10)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.3)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.7)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.10.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.9)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.3.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.26.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2022.9.24)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.0.3)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages (from typer<0.8.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.1.1)\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.4.1\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "%pip install spacy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.pipeline import make_pipeline\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# create a custom tokenizer using the spacy document processing pipeline\n",
    "def custom_tokenizer(document):\n",
    "    ppd = preprocess(document)\n",
    "    doc = nlp(ppd)\n",
    "    return [token.lemma_ for token in doc]\n",
    "\n",
    "vect = TfidfVectorizer(tokenizer=custom_tokenizer, ngram_range=(1, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the features matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the title and body into one column\n",
    "text = (df['title'] + ' ' + df['body']).values.astype('U')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create text_features using our vectorizer\n",
    "text_features = vect.fit_transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# use one hot encoder to transform the author_association to a feature set\n",
    "ohe = OneHotEncoder()\n",
    "author_association = ohe.fit_transform(df['author_association'].to_numpy().reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack\n",
    "\n",
    "# create our final features matrix by combining both sets\n",
    "X = hstack((text_features, author_association))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the target vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['Target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of feature matrix: (1000, 75923)\n",
      "Shape of target vector: (1000,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of feature matrix:\", X.shape)\n",
    "print(\"Shape of target vector:\", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 3\n",
    "Split the data into training (80%) and validation(20%) sets. We will stratify based on the label since our dataset is imbalanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, stratify=y, random_state=1)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 4\n",
    "Use cross-validate with our training set to test our model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Search for optimizing model\n",
    "\n",
    "Use grid search to find potentially better model parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Use spark_sklearn’s grid search instead:\n",
    "from spark_sklearn import GridSearchCV\n",
    "\n",
    "pipe = make_pipeline(TfidfVectorizer(tokenizer=custom_tokenizer), LogisticRegression())\n",
    "\n",
    "param_grid = {\"logisticregression__C\": [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "                \"tfidfvectorizer__ngram_range\": [(1, 1), (1, 2), (1, 3)],\n",
    "                \"tfidfvectorizer__min_df\": [0, 3, 5]}\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid, cv=5)\n",
    "grid.fit(text_train, y_train)\n",
    "\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 5\n",
    "Validate our chosen model against the validation set we saved in Stage 3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ENSF-612",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "758f2f2736c7fa9cea9c3ba7e462cc50ad01011ab3fd1554d8a168a53b5df95b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
